{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For future iterations of the project to ensure manual list of docs is not needed\n",
    "\n",
    "# import glob\n",
    "\n",
    "## Specify the directory path and file format\n",
    "# directory_path = './'\n",
    "# file_format = '*.xlsx'  # For spreadsheets\n",
    "\n",
    "## Use glob.glob to find files\n",
    "# files_list = glob.glob(directory_path + file_format)\n",
    "\n",
    "## Check list length to confirm all files are present\n",
    "# len(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []  # list for storing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multiple spreadsheets\n",
    "# Here we are using 1221-1243 data obtained on Feb 1 2024\n",
    "dataframes = [] \n",
    "\n",
    "for doc in docs:\n",
    "    repo = pd.read_excel(doc, header=1, dtype = str)  \n",
    "    dataframes.append(repo)  \n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types to ensure all is string\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of duplicated student records\n",
    "combined_df = combined_df.drop_duplicates(subset='ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm datatypes again\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for rows where 'Item Description' contains 'Amazon'\n",
    "mask_amazon = combined_df['Item Description'].str.contains('Amazon', case=False, na=False)\n",
    "\n",
    "# Create a mask for rows where 'Item Description' contains 'Boeing'\n",
    "mask_boeing = combined_df['Item Description'].str.contains('Boeing', case=False, na=False)\n",
    "\n",
    "# Create a mask for rows where 'Item Description' contains 'Boeing Spouses Waiver'\n",
    "mask_boeing_spouses_waiver = combined_df['Item Description'].str.contains('Boeing Spouses Waiver', case=False, na=False)\n",
    "\n",
    "# Filter the records. We are dropping the spouses here \n",
    "filtered_df = combined_df[(mask_amazon | (mask_boeing & ~mask_boeing_spouses_waiver))]\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm unique values for Item description to ensure that no unrelevant items are present\n",
    "unique_waivers = filtered_df['Item Description'].unique()\n",
    "unique_waivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)    # didplays entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Term (most recent first) and Item Description (ABC ascending)\n",
    "sorted_df = filtered_df.sort_values(by=['Item Term', 'Item Description'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_df)    # confirm dataframe size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.columns      # confirm columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of irrelevant columns\n",
    "columns_to_remove = ['Business Unit', 'Item Type', 'Item Amt', 'Subject', 'Catalog', 'Class Title']\n",
    "\n",
    "sorted_df = sorted_df.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm unique program values\n",
    "unique_programs = sorted_df['Prim Prog'].unique()\n",
    "unique_programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column School which will reassign programs among SBM/STC/SOEL/SHSS\n",
    "# The lists are not exhaustive and contain only options available on this report. Update accordingly upon next iteration!\n",
    "SBM_progs = ['MSTPM', 'AS-B', 'BSPM', 'BAMGT', 'UC-BU', 'MBA', 'BSBA', 'BSDA', 'DBA', 'BSIT', 'MSGSC', 'GCRTB', 'MSPM', 'MSMG', 'MSML', 'MSHA', 'UND-B']\n",
    "STC_progs = ['BSIS', 'BSDS', 'MSDS', 'BSCY', 'DIT', 'BSIT', 'BSACS', 'MSCSI', 'MSCY', 'BSAPC', 'MSISE', 'BSCYB']\n",
    "SOEL_progs = ['GCRTT', 'UC-TC', 'EDD', 'UND-T', 'BAED']\n",
    "SHSS_progs = ['MA', 'BAAP', 'AS', 'BSGS']\n",
    "\n",
    "def map_program_to_school(program):\n",
    "    if program in SBM_progs:\n",
    "        return 'SBM'\n",
    "    elif program in STC_progs:\n",
    "        return 'STC'\n",
    "    elif program in SOEL_progs:\n",
    "        return 'SOEL'\n",
    "    elif program in SHSS_progs:\n",
    "        return 'SHSS'\n",
    "    else:\n",
    "        return 'Other'  # For programs not listed\n",
    "\n",
    "sorted_df['School'] = sorted_df['Prim Prog'].apply(map_program_to_school)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get contact information and advisor name from a different query\n",
    "adv_data = pd.read_excel(\"advisor.xlsx\", header=1, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "adv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left merge based on ID\n",
    "merged_df = pd.merge(sorted_df, adv_data[['ID', 'First Enrollment Term', 'Advisor', 'Email', 'Phone']], on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display records where 'First Enrollment Term' is NaN\n",
    "nan_records = merged_df[pd.isna(merged_df['First Enrollment Term'])]\n",
    "\n",
    "len(nan_records)      # check the number of records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the dataframe format \n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table will allow us to identify students who have already graduated \n",
    "alums_data = pd.read_excel(\"contact.xlsx\", header = 1, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alums_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alums_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['Compl Term', 'Preferred Email', 'Home Email', 'Work Email']\n",
    "\n",
    "# Merge with specific columns\n",
    "final_df = pd.merge(merged_df, alums_data[['ID'] + columns_to_add], on='ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Alumni?' column based on whether 'Compl Term' is not null\n",
    "final_df['Alumni?'] = np.where(final_df['Compl Term'].notnull(), 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge existing 'Email' with 'Preferred Email', 'Home Email', and 'Work Email', remove duplicates, and NaN values\n",
    "def merge_emails(row):\n",
    "    emails = [row['Email'], row['Preferred Email'], row['Home Email'], row['Work Email']]\n",
    "    unique_emails = list(set([email for email in emails if pd.notnull(email) and email != '']))\n",
    "    return ', '.join(unique_emails)\n",
    "\n",
    "final_df['Combined Email'] = final_df.apply(merge_emails, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column order \n",
    "columns_ordered = ['Item Term', 'First Enrollment Term', 'ID', 'Name', 'School', 'Prim Prog', 'Item Description', 'Alumni?', 'Compl Term', 'Advisor', 'Combined Email', 'Phone']\n",
    "df_reordered = final_df[columns_ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reordered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records where both 'First Enrollment Term' and 'Compl Term' are NaN - needs manual lookup!\n",
    "condition = pd.isna(df_reordered['First Enrollment Term']) & pd.isna(df_reordered['Compl Term'])\n",
    "\n",
    "# Filter the DataFrame based on the condition\n",
    "rows_with_both_nan = df_reordered[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows_with_both_nan)\n",
    "# rows_with_both_nan.to_excel(\"worksheet.xlsx\", index=False)     - use to save for manual lookup in Peoplesoft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude rows_with_both_nan from the original df\n",
    "df_reordered = df_reordered[~condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_both_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once manual lookup is complete, read the file back into the notebook\n",
    "manual_entries = pd.read_excel(\"worksheet.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate records \n",
    "updated_df = pd.concat([df_reordered, manual_entries], ignore_index=True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of Item Term since we no longer need it for reference\n",
    "updated_df = updated_df.drop(['Item Term'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all records that have \"Discontinued\" in Notes to avoid students who are no longer active\n",
    "mask = updated_df['Notes'].str.strip().str.contains('Discontinued', case=False, na=False)\n",
    "\n",
    "discontinued_df = updated_df[mask]\n",
    "\n",
    "discontinued_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(discontinued_df)     # checking number of discontinued students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discontinued_df.to_excel(\"discontinued.xlsx\", index=False)    # save to a separate spreadsheet for reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an updated dataframe without discontinued students\n",
    "mask = ~updated_df['Notes'].str.strip().str.contains('Discontinued', case=False, na=False)\n",
    "\n",
    "updated_df = updated_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated records \n",
    "duplicates = updated_df.duplicated(subset='ID', keep=False)\n",
    "updated_df[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of duplicates \n",
    "df_cleaned = updated_df.drop_duplicates(subset='ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_cleaned)  # confirm total number of unique students "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status Applied for Graduation; timeframe = from January 1 2020 to February 1 2024\n",
    "commencement_eligible = pd.read_excel(\"degree_status.xlsx\", header = 1, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commencement_eligible.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for merging consistency\n",
    "commencement_eligible.rename(columns={'SID': 'ID'}, inplace=True)\n",
    "\n",
    "commencement_eligible.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates in 'commencement_eligible' based on 'ID'\n",
    "commencement_eligible = commencement_eligible.drop_duplicates(subset='ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform another merge based on ID, here we need expected graduation and cumulative GPA\n",
    "df_to_extract = pd.merge(df_cleaned, commencement_eligible[['ID', 'Expected Graduation Term', 'Cumulative GPA']], on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all students eligible for graduation\n",
    "non_empty_grad_terms = df_to_extract[\n",
    "    df_to_extract['Expected Graduation Term'].notna() |\n",
    "    df_to_extract['Notes'].str.contains('ready', case=False, na=False)\n",
    "]\n",
    "\n",
    "non_empty_grad_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df_to_extract.duplicated(subset='ID', keep=False)\n",
    "\n",
    "df_to_extract[duplicates]       # NO DUPLICATED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this query for extracting all available GPAs\n",
    "gpa_list = pd.read_excel(\"gpa_list.xlsx\", header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need GPAs dor the most recent term\n",
    "gpa_list = gpa_list.drop_duplicates(subset='ID', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for merging consistency\n",
    "gpa_list.rename(columns={'GPA': 'Cumulative GPA'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_with_gpa = pd.merge(df_to_extract, gpa_list[['ID', 'Cumulative GPA']], on='ID', how='left', suffixes=('', '_from_gpalist'))\n",
    "\n",
    "# Populate with GPA data only if the column is not empty; skip if already contains value\n",
    "\n",
    "if 'Cumulative GPA' in updated_df_with_gpa.columns and 'Cumulative GPA_from_gpalist' in updated_df_with_gpa.columns:\n",
    "    updated_df_with_gpa['Cumulative GPA'] = updated_df_with_gpa['Cumulative GPA'].fillna(updated_df_with_gpa['Cumulative GPA_from_gpalist'])\n",
    "    updated_df_with_gpa.drop(columns=['Cumulative GPA_from_gpalist'], inplace=True)\n",
    "elif 'Cumulative GPA_from_gpalist' in updated_df_with_gpa.columns:\n",
    "    updated_df_with_gpa.rename(columns={'Cumulative GPA_from_gpalist': 'Cumulative GPA'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_with_gpa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting total number of students with no GPA information\n",
    "na_cumulative_gpa_count = updated_df_with_gpa['Cumulative GPA'].isna().sum()\n",
    "print(f\"Number of rows where 'Cumulative GPA' is NaN: {na_cumulative_gpa_count} out of a total of {len(updated_df_with_gpa)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final rearrangement of columns before saving\n",
    "cl_sequence_upd = ['First Enrollment Term', 'ID', 'Name', 'School', 'Prim Prog', 'Item Description', 'Cumulative GPA', 'Alumni?', 'Expected Graduation Term', 'Compl Term', 'Advisor', 'Combined Email', 'Phone', 'Notes']\n",
    "updated_df_with_gpa = updated_df_with_gpa[cl_sequence_upd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_with_gpa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage distribution for Item Description values\n",
    "value_counts = updated_df_with_gpa['Item Description'].value_counts()\n",
    "value_percentages = (value_counts / len(updated_df_with_gpa)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "updated_df_with_gpa.to_excel(\"results.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
